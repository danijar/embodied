import functools
import os
import re
import threading
from concurrent.futures import ThreadPoolExecutor
from functools import partial as bind

import chex
import embodied
import jax
import jax.numpy as jnp
import numpy as np
from jax.experimental import checkify
from jax._src import checkify as src

from . import jaxutils
from . import ninjax as nj

tree_map = jax.tree_util.tree_map
tree_leaves = jax.tree_util.tree_leaves

def patched(self):
  return (
      f'nan generated by primitive: {self.prim} at:\n\n' +
      f'{self.traceback_info}')
src.NaNError.__str__ = patched

def patched():
  return str(src.source_info_util.summarize(
      src.source_info_util.current(), 30))
src.summary = patched


def Wrapper(agent_cls):
  class Agent(JAXAgent):
    configs = agent_cls.configs
    inner = agent_cls
    def __init__(self, *args, **kwargs):
      super().__init__(agent_cls, *args, **kwargs)
  return Agent


class JAXAgent(embodied.Agent):

  def __init__(self, agent_cls, obs_space, act_space, config):
    print('Observation space')
    [print(f'  {k:<16} {v}') for k, v in obs_space.items()]
    print('Action space')
    [print(f'  {k:<16} {v}') for k, v in act_space.items()]

    self.obs_space = obs_space
    self.act_space = act_space
    self.config = config
    self.jaxcfg = config.jax
    self.logdir = embodied.Path(config.logdir)
    self._setup()
    self.agent = agent_cls(obs_space, act_space, config, name='agent')
    self.rng = np.random.default_rng(config.seed)
    self.keys = [
        k for k in list(obs_space.keys()) + list(act_space.keys())
        if not k.startswith('_') and k != 'reset']

    self.train_static = {}
    self.policy_static = {'mode': 'train'}

    available = jax.devices(self.jaxcfg.platform)
    print(f'JAX devices ({jax.local_device_count()}):', available)
    if self.jaxcfg.assert_num_devices > 0:
      assert len(available) == self.jaxcfg.assert_num_devices, (
          available, len(available), self.jaxcfg.assert_num_devices)
    self.policy_devices = [available[i] for i in self.jaxcfg.policy_devices]
    self.train_devices = [available[i] for i in self.jaxcfg.train_devices]
    print('Policy devices:', ', '.join([str(x) for x in self.policy_devices]))
    print('Train devices: ', ', '.join([str(x) for x in self.train_devices]))
    self.single_device = (self.policy_devices == self.train_devices) and (
        len(self.policy_devices) == 1)

    self.outs_worker = ThreadPoolExecutor(1, 'jaxagent_outs')
    self.mets_worker = ThreadPoolExecutor(1, 'jaxagent_mets')
    self.sync_worker = ThreadPoolExecutor(1, 'jaxagent_sync')
    self.outs_promise = None
    self.mets_promise = None
    self.sync_promise = None

    self._transform()
    self.params_lock = threading.Lock()
    with embodied.timer.section('agent_init'):
      self.params = self._init_varibs(obs_space, act_space)
    self.updates = embodied.Counter()

    pattern = re.compile(self.jaxcfg.policy_keys)
    self.policy_keys = [k for k in self.params.keys() if pattern.search(k)]
    self.should_sync = embodied.when.Every(self.jaxcfg.sync_every)
    self.prev_params = {k: self.params[k] for k in self.policy_keys}
    if not self.single_device:
      self.policy_params = self._copy_params(self.prev_params)

  def init_policy(self, batch_size):
    varibs = self.prev_params if self.single_device else self.policy_params
    rng = self._next_rngs(self.policy_devices)
    is_first = np.ones(batch_size, bool)
    is_first = self._convert_inps(is_first, self.policy_devices)
    state, _ = self._init_policy(varibs, rng, is_first)
    state = self._convert_outs(state, self.policy_devices)
    return state

  def init_train(self, batch_size):
    rng = self._next_rngs(self.train_devices)
    is_first = np.ones((batch_size, self.config.batch_length), bool)
    is_first = self._convert_inps(is_first, self.train_devices)
    state, _ = self._init_train(self.params, rng, is_first)
    return state

  @embodied.timer.section('jaxagent_policy')
  def policy(self, obs, state, **kwargs):
    assert state is not None
    # if state is None:
    #   state = self.init_policy(len(obs['is_first']))
    obs = self._filter_data(obs)

    with embodied.timer.section('prepare_state'):
      state = tree_map(
          lambda x: np.asarray(x) if isinstance(x, list) else x,
          state, is_leaf=lambda x: isinstance(x, list))
    with embodied.timer.section('upload_inputs'):
      obs, state = self._convert_inps((obs, state), self.policy_devices)
      rng = self._next_rngs(self.policy_devices)

    kwargs = {**self.policy_static, **kwargs}
    with embodied.timer.section('agent_policy'):
      varibs = self.prev_params if self.single_device else self.policy_params
      (outs, state), _ = self._policy(
          varibs, rng=rng, obs=obs, carry=state, **kwargs)

    if not self.single_device:
      with embodied.timer.section('swap_varibs'):
        if self.sync_promise and self.sync_promise.done():
          self.policy_params = self.sync_promise.result()
          self.sync_promise = None
    with embodied.timer.section('fetch_outputs'):
      outs, state = self._convert_outs((outs, state), self.policy_devices)
    return outs, state

  @embodied.timer.section('jaxagent_train')
  def train(self, data, state, **kwargs):
    assert state is not None
    data = data.copy()
    rng = data.pop('rng')
    data = self._filter_data(data)
    kwargs = {**self.train_static, **kwargs}

    self.prev_params = {k: self.params[k] for k in self.policy_keys}
    with embodied.timer.section('agent_train'):
      items = self.params.items()
      allocated = {k: v for k, v in items if k in self.policy_keys}
      donated = {k: v for k, v in items if k not in self.policy_keys}
      with self.params_lock:
        (outs, state, mets), self.params = self._train(
            allocated, donated=donated, rng=rng, data=data, carry=state,
            **kwargs)

    self.updates.increment()

    if not self.single_device:
      if not self.sync_promise and self.should_sync(self.updates):
        self.sync_promise = self.sync_worker.submit(
            self._copy_params, self.prev_params, block=True)

    return_outs = {}
    if self.outs_promise:
      return_outs = self.outs_promise.result()
    self.outs_promise = self.outs_worker.submit(
        self._convert_outs, outs, self.train_devices)

    return_mets = {}
    if self.mets_promise and self.mets_promise.done():
      return_mets = self.mets_promise.result()
      self.mets_promise = None
    if not self.mets_promise:
      # Only request metrics if we aren't currently waiting for previous
      # metrics. This means we'll skip the metrics of some training steps if
      # fetching them from device would slow down the training loop.
      self.mets_promise = self.mets_worker.submit(
          self._convert_mets, mets, self.train_devices)

    if self.jaxcfg.profiler:
      outdir, copyto = self.logdir, None
      if str(outdir).startswith(('gs://', '/gcs/')):
        copyto = outdir
        outdir = embodied.Path('/tmp/profiler')
        outdir.mkdirs()
      if self.updates == 100:
        embodied.print(f'Start JAX profiler: {str(outdir)}', 'yellow')
        jax.profiler.start_trace(str(outdir))
      if self.updates == 140:
        from embodied.core import path as pathlib
        embodied.print('Stop JAX profiler', 'yellow')
        jax.profiler.stop_trace()
        if copyto:
          pathlib.GFilePath(outdir).copy(copyto)
          print(f'Copied profiler result {outdir} to {copyto}')

    return return_outs, state, return_mets

  @embodied.timer.section('jaxagent_report')
  def report(self, data):
    data = data.copy()
    rng = data.pop('rng')
    data = self._filter_data(data)
    with embodied.timer.section('agent_report'):
      mets, _ = self._report(self.params, rng, data)
    mets = self._convert_mets(mets, self.train_devices)
    return mets

  def dataset(self, generator):
    def transform(data):
      return {
          **self._convert_inps(data, self.train_devices),
          'rng': self._next_rngs(self.train_devices)}
    return embodied.Prefetch(generator, transform)

  @embodied.timer.section('jaxagent_save')
  def save(self):
    with self.params_lock:
      if len(self.train_devices) > 1:
        varibs = tree_map(lambda x: x[0], self.params)
      else:
        varibs = self.params
      return jax.device_get(varibs)

  @embodied.timer.section('jaxagent_load')
  def load(self, state):
    with self.params_lock:
      del self.prev_params
      if len(self.train_devices) == 1:
        chex.assert_trees_all_equal_shapes(self.params, state)
        del self.params
        self.params = jax.device_put(state, self.train_devices[0])
      else:
        chex.assert_trees_all_equal_shapes(
            tree_map(lambda x: x[0], self.params), state)
        del self.params
        self.params = jax.device_put_replicated(state, self.train_devices)
      self.prev_params = {k: self.params[k] for k in self.policy_keys}
      if not self.single_device:
        self.policy_params = self._copy_params(self.prev_params)

  def _setup(self):
    try:
      import tensorflow as tf
      tf.config.set_visible_devices([], 'GPU')
      tf.config.set_visible_devices([], 'TPU')
    except Exception as e:
      print('Could not disable TensorFlow devices:', e)
    if not self.jaxcfg.prealloc:
      os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
    xla_flags = []
    if self.jaxcfg.logical_cpus:
      count = self.jaxcfg.logical_cpus
      xla_flags.append(f'--xla_force_host_platform_device_count={count}')
    if xla_flags:
      os.environ['XLA_FLAGS'] = ' '.join(xla_flags)
    jax.config.update('jax_platform_name', self.jaxcfg.platform)
    jax.config.update('jax_disable_jit', not self.jaxcfg.jit)
    # jax.config.update('jax_debug_nans', self.jaxcfg.debug_nans)
    if self.jaxcfg.transfer_guard:
      jax.config.update('jax_transfer_guard', 'disallow')
    if self.jaxcfg.platform == 'cpu':
      jax.config.update('jax_disable_most_optimizations', self.jaxcfg.debug)
    jaxutils.COMPUTE_DTYPE = getattr(jnp, self.jaxcfg.compute_dtype)
    jaxutils.PARAM_DTYPE = getattr(jnp, self.jaxcfg.param_dtype)

  def _transform(self):
    self._init_train = nj.pure(lambda x: self.agent.train_initial(len(x)))
    self._init_policy = nj.pure(lambda x: self.agent.policy_initial(len(x)))
    self._train = nj.pure(self.agent.train)
    self._policy = nj.pure(self.agent.policy)
    self._report = nj.pure(self.agent.report)

    self._init_policy = bind(self._init_policy, modify=False)
    self._train = lambda state, donated, rng, data, fn=self._train, **kw: (
        fn({**state, **donated}, rng=rng, data=data, **kw))
    self._policy = bind(self._policy, modify=False)
    self._report = bind(self._report, modify=False)

    train_static = list(self.train_static.keys()) + ['create', 'modify']
    policy_static = list(self.policy_static.keys()) + ['create', 'modify']
    train_donate = ['donated', 'rng', 'data', 'carry']
    policy_donate = ['rng', 'obs', 'carry']

    if len(self.train_devices) == 1:
      kw = dict(device=self.train_devices[0])
      self._init_train = nj.jit(self._init_train, **kw)
      self._train = nj.jit(
          self._train, static=train_static, donate=train_donate, **kw)
      self._report = nj.jit(self._report, **kw)
    else:
      kw = dict(devices=self.train_devices)
      self._init_train = nj.pmap(self._init_train, 'i', **kw)
      self._train = nj.pmap(
          self._train, 'i', static=train_static, donate=train_donate, **kw)
      self._report = nj.pmap(self._report, 'i', **kw)

    if len(self.policy_devices) == 1:
      kw = dict(device=self.policy_devices[0])
      self._init_policy = nj.jit(self._init_policy, **kw)
      self._policy = nj.jit(
          self._policy, static=policy_static, donate=policy_donate, **kw)
    else:
      kw = dict(devices=self.policy_devices)
      self._init_policy = nj.pmap(self._init_policy, 'i', **kw)
      self._policy = nj.pmap(
          self._policy, 'i', static=policy_static, donate=policy_donate, **kw)

    self._init_policy = bind(self._init_policy, init=False)
    self._policy = bind(self._policy, init=False)
    self._report = bind(self._report, init=False)

    if self.jaxcfg.checks:
      assert self.config.run.actor_threads == 1, 'chex is not thread-safe'
      jaxutils.ENABLE_CHECKS = True
      wrap = nj.static_support(self._checkify)
      self._policy = wrap(self._policy, static=policy_static)
      self._train = wrap(self._train, static=train_static + ['init'])
      self._report = wrap(self._report)
      self._init_train = wrap(self._init_train)
      self._init_policy = wrap(self._init_policy)

  def _checkify(self, fun):
    fun = chex.chexify(fun, True, checkify.user_checks)
    @functools.wraps(fun)
    def transformed(*args, **kwargs):
      try:
        return fun(*args, **kwargs)
      except Exception:
        chex.block_until_chexify_assertions_complete()
        raise
    return transformed

  def _convert_inps(self, value, devices, rng=False, block=False):
    if len(devices) == 1:
      value = jax.device_put(value, devices[0])
    else:
      D = len(devices)
      try:
        value = tree_map(lambda x: x.reshape((D, -1, *x.shape[1:])), value)
      except Exception as e:
        shapes = tree_map(lambda x: x.shape, value)
        msg = f'Shapes are not divislbe by {D} devices: {shapes}'
        raise ValueError(msg) from e
      shards = [tree_map(lambda x: x[d], value) for d in range(D)]
      value = jax.device_put_sharded(shards, devices)
    if rng:
      value['rng'] = self._next_rngs(devices)
    if block:
      jax.block_until_ready(value)
    return value

  def _convert_outs(self, value, devices):
    value = jax.device_get(value)
    if len(devices) > 1:
      value = tree_map(lambda x: x.reshape((-1,) + x.shape[2:]), value)
    return value

  def _convert_mets(self, value, devices):
    if len(devices) > 1:
      value = tree_map(lambda x: x[0], value)
    return jax.device_get(value)

  def _next_rngs(self, devices, mirror=False):
    high = np.iinfo(np.uint32).max
    if len(devices) == 1:
      return jax.device_put(
          self.rng.integers(0, high, (2,), np.uint32), devices[0])
    elif mirror:
      return jax.device_put_replicated(
          self.rng.integers(0, high, (2,), np.uint32), devices)
    else:
      return jax.device_put_sharded(
          list(self.rng.integers(0, high, (len(devices), 2), np.uint32)),
          devices)

  def _init_varibs(self, obs_space, act_space):
    varibs = {}
    rng = self._next_rngs(self.train_devices, mirror=True)
    dims = (self.config.batch_size, self.config.batch_length)
    data = self._dummy_batch({**obs_space, **act_space}, dims)
    data = self._convert_inps(data, self.train_devices)
    state, varibs = self._init_train(varibs, rng, data['is_first'])
    varibs = self._train(
        varibs, rng=rng, data=data, carry=state, donated={}, apply=False)
    # obs = self._dummy_batch(obs_space, (1,))
    # state, varibs = self._init_policy(varibs, rng, obs['is_first'])
    # varibs = self._policy(
    #     varibs, rng, obs, state, mode='train', init_only=True)
    # print('Params:', embodied.format(varibs))
    return varibs

  @embodied.timer.section('copy_varibs')
  def _copy_params(self, varibs, block=False):
    if self.single_device:
      return varibs
    if len(self.train_devices) > 1:
      varibs = tree_map(lambda x: x[0].device_buffer, varibs)
    if len(self.policy_devices) == 1:
      varibs = jax.device_put(varibs, self.policy_devices[0])
    else:
      varibs = jax.device_put_replicated(varibs, self.policy_devices)
    if block:
      jax.block_until_ready(varibs)
    return varibs

  def _filter_data(self, data):
    return {k: v for k, v in data.items() if k in self.keys}

  def _dummy_batch(self, spaces, batch_dims):
    spaces = [(k, v) for k, v in spaces.items()]
    data = {k: np.zeros(v.shape, v.dtype) for k, v in spaces}
    data = self._filter_data(data)
    for dim in reversed(batch_dims):
      data = {k: np.repeat(v[None], dim, axis=0) for k, v in data.items()}
    return data
