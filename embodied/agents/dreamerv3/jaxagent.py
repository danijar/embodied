import concurrent.futures
import functools
import os

import chex
import embodied
import jax
import jax.numpy as jnp
import numpy as np
from jax.experimental import checkify
from jax._src import checkify as src

from . import jaxutils
from . import ninjax as nj

tree_map = jax.tree_util.tree_map
tree_leaves = jax.tree_util.tree_leaves

def patched(self):
  return (
      f'nan generated by primitive: {self.prim} at:\n\n' +
      f'{self.traceback_info}')
src.NaNError.__str__ = patched

def patched():
  return str(src.source_info_util.summarize(
      src.source_info_util.current(), 30))
src.summary = patched


def Wrapper(agent_cls):
  class Agent(JAXAgent):
    configs = agent_cls.configs
    inner = agent_cls
    def __init__(self, *args, **kwargs):
      super().__init__(agent_cls, *args, **kwargs)
  return Agent


class JAXAgent(embodied.Agent):

  def __init__(self, agent_cls, obs_space, act_space, step, config):
    self.config = config
    self.jaxcfg = config.jax
    self.logdir = embodied.Path(config.logdir)
    self._setup()
    self.agent = agent_cls(obs_space, act_space, step, config, name='agent')
    self.rng = np.random.default_rng(config.seed)

    available = jax.devices(self.jaxcfg.platform)
    self.policy_devices = [available[i] for i in self.jaxcfg.policy_devices]
    self.train_devices = [available[i] for i in self.jaxcfg.train_devices]
    self.single_device = (self.policy_devices == self.train_devices) and (
        len(self.policy_devices) == 1)
    print(f'JAX devices ({jax.local_device_count()}):', available)
    print('Policy devices:', ', '.join([str(x) for x in self.policy_devices]))
    print('Train devices: ', ', '.join([str(x) for x in self.train_devices]))

    self._transform()
    self.varibs = self._init_varibs(obs_space, act_space)
    self.updates = embodied.Counter()

    self.outs_worker = concurrent.futures.ThreadPoolExecutor(1)
    self.mets_worker = concurrent.futures.ThreadPoolExecutor(1)
    self.sync_worker = concurrent.futures.ThreadPoolExecutor(1)
    self.outs_promise = None
    self.mets_promise = None
    self.sync_promise = None

    self.should_sync = embodied.when.Every(self.jaxcfg.sync_every)
    if not self.single_device:
      self.policy_varibs = self._copy_varibs(self.varibs)

  def policy(self, obs, state=None, mode='train'):
    obs = self._filter_data(obs.copy())
    obs = self._convert_inps(obs, self.policy_devices)
    rng = self._next_rngs(self.policy_devices)
    varibs = self.varibs if self.single_device else self.policy_varibs

    if state is None:
      state, _ = self._init_policy(varibs, rng, obs['is_first'])
    else:
      state = tree_map(
          np.asarray, state, is_leaf=lambda x: isinstance(x, list))
      state = self._convert_inps(state, self.policy_devices)

    (outs, state), _ = self._policy(varibs, rng, obs, state, mode=mode)
    if not self.single_device:
      if self.sync_promise and self.sync_promise.done():
        self.policy_varibs = self.sync_promise.result()
        self.sync_promise = None
    outs = self._convert_outs(outs, self.policy_devices)

    # TODO: Consider keeping policy states in accelerator memory.
    state = self._convert_outs(state, self.policy_devices)

    return outs, state

  def train(self, data, state=None):
    data = self._filter_data(data.copy())
    rng = data.pop('rng')
    if state is None:
      rng = self._next_rngs(self.train_devices)
      state, self.varibs = self._init_train(self.varibs, rng, data['is_first'])
    prev_varibs = self.varibs

    (outs, state, mets), self.varibs = self._train(
        self.varibs, rng, data, state)
    self.updates.increment()

    if not self.single_device:
      if not self.sync_promise and self.should_sync(self.updates):
        self.sync_promise = self.sync_worker.submit(
            self._copy_varibs, prev_varibs, block=True)

    return_outs = {}
    if self.outs_promise:
      return_outs = self.outs_promise.result()
    self.outs_promise = self.outs_worker.submit(
        self._convert_outs, outs, self.train_devices)

    return_mets = {}
    if self.mets_promise and self.mets_promise.done():
      return_mets = self.mets_promise.result()
      self.mets_promise = None
    if not self.mets_promise:
      # Only request metrics if we aren't currently waiting for previous
      # metrics. This means we'll skip the metrics of some training steps if
      # fetching them from device would slow down the training loop.
      self.mets_promise = self.mets_worker.submit(
          self._convert_mets, mets, self.train_devices)

    if self.jaxcfg.profiler:
      outdir, copyto = self.logdir, None
      if str(outdir).startswith(('gs://', '/gcs/')):
        copyto = outdir
        outdir = embodied.Path('/tmp/profiler')
        outdir.mkdirs()
      if self.updates == 100:
        print(f'Start JAX profiler ({str(outdir)})')
        jax.profiler.start_trace(str(outdir))
      if self.updates == 140:
        from embodied.core import path as pathlib
        print('Stop JAX profiler')
        jax.profiler.stop_trace()
        if copyto:
          pathlib.GFilePath(outdir).copy(copyto)
          print(f'Copied profiler result {outdir} to {copyto}')

    return return_outs, state, return_mets

  def report(self, data):
    data = self._filter_data(data.copy())
    # TODO: We could also do the same pipelining optimization used in train()
    # but it doesn't really matter because report() is not called as often.
    data = data.copy()
    rng = data.pop('rng')
    mets, _ = self._report(self.varibs, rng, data)
    mets = self._convert_mets(mets, self.train_devices)
    return mets

  def dataset(self, generator):
    batcher = embodied.Batcher(
        sources=[generator] * self.config.batch_size,
        workers=self.config.data_loaders,
        postprocess=lambda x: {
            **self._convert_inps(x, self.train_devices),
            'rng': self._next_rngs(self.train_devices)},
        prefetch_source=4,
        prefetch_batch=1)
    return batcher()

  def save(self):
    if len(self.train_devices) > 1:
      varibs = tree_map(lambda x: x[0], self.varibs)
    else:
      varibs = self.varibs
    return jax.device_get(varibs)

  def load(self, state):
    chex.assert_trees_all_equal_shapes(self.varibs, state)
    if len(self.train_devices) == 1:
      self.varibs = jax.device_put(state, self.train_devices[0])
    else:
      self.varibs = jax.device_put_replicated(state, self.train_devices)
    if not self.single_device:
      self.policy_varibs = self._copy_varibs(self.varibs)

  def _setup(self):
    try:
      import tensorflow as tf
      tf.config.set_visible_devices([], 'GPU')
      tf.config.set_visible_devices([], 'TPU')
    except Exception as e:
      print('Could not disable TensorFlow devices:', e)
    if not self.jaxcfg.prealloc:
      os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
    xla_flags = []
    if self.jaxcfg.logical_cpus:
      count = self.jaxcfg.logical_cpus
      xla_flags.append(f'--xla_force_host_platform_device_count={count}')
    if xla_flags:
      os.environ['XLA_FLAGS'] = ' '.join(xla_flags)
    jax.config.update('jax_platform_name', self.jaxcfg.platform)
    jax.config.update('jax_disable_jit', not self.jaxcfg.jit)
    # jax.config.update('jax_debug_nans', self.jaxcfg.debug_nans)
    if self.jaxcfg.transfer_guard:
      jax.config.update('jax_transfer_guard', 'disallow')
    if self.jaxcfg.platform == 'cpu':
      jax.config.update('jax_disable_most_optimizations', self.jaxcfg.debug)
    jaxutils.COMPUTE_DTYPE = getattr(jnp, self.jaxcfg.precision)

  def _transform(self):
    self._init_policy = nj.pure(lambda x: self.agent.policy_initial(len(x)))
    self._init_train = nj.pure(lambda x: self.agent.train_initial(len(x)))
    self._policy = nj.pure(self.agent.policy)
    self._train = nj.pure(self.agent.train)
    self._report = nj.pure(self.agent.report)
    if len(self.train_devices) == 1:
      kw = dict(device=self.train_devices[0])
      self._init_train = nj.jit(self._init_train, **kw)
      self._train = nj.jit(self._train, **kw)
      self._report = nj.jit(self._report, **kw)
    else:
      kw = dict(devices=self.train_devices)
      self._init_train = nj.pmap(self._init_train, 'i', **kw)
      self._train = nj.pmap(self._train, 'i', **kw)
      self._report = nj.pmap(self._report, 'i', **kw)
    if len(self.policy_devices) == 1:
      kw = dict(device=self.policy_devices[0])
      self._init_policy = nj.jit(self._init_policy, **kw)
      self._policy = nj.jit(self._policy, static=['mode'], **kw)
    else:
      kw = dict(devices=self.policy_devices)
      self._init_policy = nj.pmap(self._init_policy, 'i', **kw)
      self._policy = nj.pmap(self._policy, 'i', static=['mode'], **kw)
    if self.jaxcfg.checks:
      assert self.config.run.actor_threads == 1, 'chex is not thread-safe'
      jaxutils.ENABLE_CHECKS = True
      self._policy = jaxutils.transform_with_static(
          self._checkify, 'mode')(self._policy)
      self._train = jaxutils.transform_with_static(
          self._checkify, 'init_only')(self._train)
      self._report = self._checkify(self._report)
      self._init_train = self._checkify(self._init_train)
      self._init_policy = self._checkify(self._init_policy)

  def _checkify(self, fn):
    fn = chex.chexify(fn, True, checkify.user_checks)
    @functools.wraps(fn)
    def transformed(*args, **kwargs):
      try:
        return fn(*args, **kwargs)
      except Exception:
        chex.block_until_chexify_assertions_complete()
        raise
    return transformed

  def _convert_inps(self, value, devices, rng=False, block=False):
    if len(devices) == 1:
      value = jax.device_put(value, devices[0])
    else:
      check = tree_map(lambda x: len(x) % len(devices) == 0, value)
      if not all(jax.tree_util.tree_leaves(check)):
        shapes = tree_map(lambda x: x.shape, value)
        raise ValueError(
            f'Batch must by divisible by {len(devices)} devices: {shapes}')
      value = tree_map(
          lambda x: x.reshape((len(devices), -1) + x.shape[1:]), value)
      shards = []
      for i in range(len(devices)):
        shards.append(tree_map(lambda x: x[i], value))
      value = jax.device_put_sharded(shards, devices)
    if rng:
      value['rng'] = self._next_rngs(devices)
    if block:
      jax.block_until_ready(value)
    return value

  def _convert_outs(self, value, devices):
    value = jax.device_get(value)
    if len(devices) > 1:
      value = tree_map(lambda x: x.reshape((-1,) + x.shape[2:]), value)
    return value

  def _convert_mets(self, value, devices):
    if len(devices) > 1:
      value = tree_map(lambda x: x[0], value)
    return jax.device_get(value)

  def _next_rngs(self, devices, mirror=False):
    high = np.iinfo(np.uint32).max
    if len(devices) == 1:
      return jax.device_put(
          self.rng.integers(0, high, (2,), np.uint32), devices[0])
    elif mirror:
      return jax.device_put_replicated(
          self.rng.integers(0, high, (2,), np.uint32), devices)
    else:
      return jax.device_put_sharded(
          list(self.rng.integers(0, high, (len(devices), 2), np.uint32)),
          devices)

  def _init_varibs(self, obs_space, act_space):
    varibs = {}
    rng = self._next_rngs(self.train_devices, mirror=True)
    dims = (self.config.batch_size, self.config.batch_length)
    data = self._dummy_batch({**obs_space, **act_space}, dims)
    data = self._convert_inps(data, self.train_devices)
    state, varibs = self._init_train(varibs, rng, data['is_first'])
    varibs = self._train(varibs, rng, data, state, init_only=True)
    # obs = self._dummy_batch(obs_space, (1,))
    # state, varibs = self._init_policy(varibs, rng, obs['is_first'])
    # varibs = self._policy(
    #     varibs, rng, obs, state, mode='train', init_only=True)
    return varibs

  def _copy_varibs(self, varibs, block=False):
    if self.single_device:
      return varibs
    if len(self.train_devices) > 1:
      varibs = tree_map(lambda x: x[0].device_buffer, self.varibs)
    if len(self.policy_devices) == 1:
      varibs = jax.device_put(varibs, self.policy_devices[0])
    else:
      varibs = jax.device_put_replicated(varibs, self.policy_devices)
    if block:
      jax.block_until_ready(varibs)
    return varibs

  def _filter_data(self, data):
    return {
        k: v for k, v in data.items()
        if not k.startswith('log_') and k != 'reset'}

  def _dummy_batch(self, spaces, batch_dims):
    spaces = [(k, v) for k, v in spaces.items()]
    data = {k: np.zeros(v.shape, v.dtype) for k, v in spaces}
    data = self._filter_data(data)
    for dim in reversed(batch_dims):
      data = {k: np.repeat(v[None], dim, axis=0) for k, v in data.items()}
    return data
