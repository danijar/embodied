defaults:

  seed: 0
  method: name
  task: dummy_disc
  logdir: /dev/null
  eval_dir: ''
  filter: 'score|length|fps|ratio|train/.*loss_mean|randomness_mean'
  tensorboard_videos: True

  replay:
    size: 1e5
    online: True
    fracs: {uniform: 1.0, priority: 0.0, recency: 0.0}
    prio: {exponent: 0.8, maxfrac: 0.5, initial: inf, zero_on_sample: True}
    priosignal: model
    recexp: 1.0
    chunksize: 1024
    save_wait: False

  jax:
    platform: gpu
    jit: True
    compute_dtype: float32
    param_dtype: float32
    prealloc: True
    checks: False
    logical_cpus: 0
    debug: False
    policy_devices: [0]
    train_devices: [0]
    sync_every: 10
    profiler: False
    transfer_guard: True
    nvidia_flags: False
    xla_dump: False
    assert_num_devices: -1
    fetch_policy_carry: False

  run:
    script: train
    steps: 1e10
    duration: 0
    num_envs: 4
    num_envs_eval: 4
    expl_until: 0
    log_every: 120
    save_every: 900
    eval_every: 1e6
    eval_initial: True
    eval_eps: 1
    eval_samples: 1
    train_ratio: 3.0
    train_fill: 1000
    eval_fill: 0
    log_zeros: True
    log_keys_video: [image]
    log_keys_sum: '^$'
    log_keys_avg: '^$'
    log_keys_max: '^$'
    log_video_fps: 20
    log_video_streams: 4
    log_episode_timeout: 60
    from_checkpoint: ''
    actor_addr: 'tcp://localhost:{auto}'
    replay_addr: 'tcp://localhost:{auto}'
    logger_addr: 'tcp://localhost:{auto}'
    actor_batch: 32
    actor_threads: 4
    replica: -1
    ipv6: False
    usage: {psutil: True, nvsmi: True, gputil: False, malloc: False, gc: False}
    timer: True  # A bit slow but useful.
    driver_parallel: True
    agent_process: False
    remote_replay: False

  wrapper: {length: 0, reset: True, discretize: 0, checks: True}
  env:
    atari: {size: [64, 64], repeat: 4, sticky: True, gray: True, actions: all, lives: unused, noops: 30, autostart: False, pooling: 2, aggregate: max, resize: pillow, clip_reward: False}
    crafter: {size: [64, 64], logs: False}
    atari100k: {size: [64, 64], repeat: 4, sticky: False, gray: False, actions: needed, lives: unused, noops: 30, autostart: False, resize: pillow, clip_reward: False}
    dmlab: {size: [64, 64], repeat: 4, episodic: True, use_seed: True}
    minecraft: {size: [64, 64], break_speed: 100.0, logs: False}
    dmc: {size: [64, 64], repeat: 2, image: True, camera: -1}
    loconav: {size: [64, 64], repeat: 2, camera: -1}

  random_agent: False
  batch_size: 64
  batch_length: 65
  batch_length_eval: 33
  replay_length: 0
  replay_length_eval: 0
  replay_context: 1
  enc:
    spaces: '.*'
    typ: impala
    impala: {depth: 32, mults: [1, 2, 2], layers: 5, units: 1024, act: relu, norm: none, winit: normal, fan: in, symlog: True}
  recurrent: True
  gru: {units: 1024, norm: layer, winit: normal, fan: in}
  policy: {layers: 0, units: 1024, act: relu, norm: layer, minstd: 0.1, maxstd: 1.0, outscale: 0.0, unimix: 0.0, inputs: [feat], winit: normal, fan: in}
  value: {layers: 0, units: 1024, act: relu, norm: layer, dist: mse, outscale: 0.0, inputs: [feat], winit: normal, fan: in, bins: 255}
  ppo_losses: {actent: 1e-2, hor: 200, lam: 0.8, trclip: 0.2, tarclip: 10.0}
  loss_scales: {policy: 1.0, value: 0.5}
  valnorm: {impl: mean_std, rate: 0.01, limit: 1e-8}
  advnorm: {impl: mean_std, rate: 0.01, limit: 1e-8}
  opt: {opt: adam, lr: 3e-4, eps: 1e-7, clip: 10.0, wd: 0.0, warmup: 1000, agc: 0.0, postagc: 0.0}
  policy_dist_disc: onehot
  policy_dist_cont: normal_sigmoidstd
  gru_actions: True

openai:
  opt.eps: 1e-8
  opt.clip: 0.5
  policy.outscale: 0.01
  value.outscale: 1.0
  policy_dist_cont: normal_logstd
  horizon: 1000
  return_lambda: 0.95
  gru_actions: False
  replay.size: 1e4

minecraft:
  task: minecraft_diamond
  run:
    num_envs: 16
    eval_fill: 1e5
    log_keys_max: '^log_inventory.*'
  enc.spaces: 'image|inventory|inventory_max|equipped|health|hunger|breath|reward'

dmlab:
  task: dmlab_explore_goal_locations_small
  enc.spaces: 'image'
  run:
    num_envs: 8

atari:
  task: atari_pong
  run:
    steps: 5.5e7
    eval_eps: 10
    num_envs: 8
  enc.spaces: 'image'

procgen:
  task: procgen_bigfish
  run.steps: 5.1e7
  enc.spaces: 'image'

atari100k:
  task: atari_pong
  run:
    script: train_eval
    steps: 1.5e5
    num_envs: 1
    eval_every: 1e5
    eval_initial: False
    eval_eps: 100
  jax.precision: float32
  enc.spaces: 'image'

crafter:
  task: crafter_reward
  run:
    num_envs: 1
    log_keys_max: '^log_achievement_.*'
    log_keys_sum: '^log_reward$'
    log_video_fps: 10
  enc.spaces: 'image'

dmc_vision:
  task: dmc_walker_walk
  enc.spaces: 'image'

dmc_proprio:
  task: dmc_walker_walk
  env.dmc.image: False

bsuite:
  task: bsuite_mnist/0
  run:
    num_envs: 1
    script: train

loconav:
  task: loconav_ant_maze_m
  env.loconav.repeat: 1
  run:
    log_keys_max: '^log_.*'
  enc.spaces: '.*'

multicpu:
  jax:
    logical_cpus: 8
    policy_devices: [0, 1]
    train_devices: [2, 3, 4, 5, 6, 7]
  run:
    num_envs: 8
    actor_batch: 4
  batch_size: 12
  batch_length: 10

debug:

  jax: {debug: True, jit: True, prealloc: False, platform: cpu, profiler: False, checks: False, compute_dtype: bfloat16}
  wrapper: {length: 100}
  run: {num_envs: 4, eval_every: 1000, log_every: 5, save_every: 15, train_ratio: 8, actor_batch: 2, driver_parallel: False, train_fill: 100}
  batch_size: 8
  batch_length: 12
  batch_length_eval: 12
  enc.impala.depth: 4
  gru.units: 32
  .*\.(mlp_)?layers: 2
  .*\.(mlp_)?units: 16
  .*\.wd$: 0.0
